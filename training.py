import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
import h5py
import random
from models import generator, discriminator
from preprocessing import mask_images

tf.enable_eager_execution()

# get generator and discriminator model
gen = generator(input_shape=(64, 64, 3))
dis = discriminator(input_shape=(64, 64, 3))

# optimizers for both
gen_optimizer = tf.train.AdamOptimizer(1e-4)
dis_optimizer = tf.train.AdamOptimizer(1e-5)

# dataset
(train_x, _), (_, _) = tf.keras.datasets.cifar10.load_data()
train_x = (train_x / 255.0).astype(dtype=np.float32)
# resizing (cifar10 is 32x32x3, but vgg need 64x64x3)
train_x = tf.image.resize_images(train_x, size=(64, 64))

# 10x10 random mask for images
mask = tf.ones_like(train_x).numpy()
for m in mask:
    size = 10
    x = random.randint(10, 40)
    y = random.randint(10, 40)
    m[y:y + size, x:x + size] = 0


masked_train_x = mask_images(images=train_x, mask=mask)

#parameters
epoch_num = 20
batch_size = 128

for episode in range(epoch_num):
    # arrays for epoch summary
    gen_avg_loss = np.array(0, dtype=np.float32)
    perceptual_accuracy = np.array(0, dtype=np.float32)  # in 0..1 range
    dis_avg_real_accuracy = np.array(0, dtype=np.float32)  # in 0..1 range
    dis_avg_fake_accuracy = np.array(0, dtype=np.float32)  # in 0..1 range

    for step in range(int(len(train_x) / batch_size)):
        real_images = train_x[step * batch_size: (step + 1) * batch_size]
        batch_mask = mask[step * batch_size: (step + 1) * batch_size]
        masked_images = masked_train_x[step * batch_size: (step + 1) * batch_size]

        with tf.GradientTape() as gen_tape, tf.GradientTape() as dis_tape:
            # we want this to look like normal image, no holes
            gen_output = gen(masked_images, training=True)

            # discriminator prediction of photos, whether they are real or generated by generator
            dis_fake_output = dis(gen_output, training=True)
            dis_real_output = dis(real_images, training=True)

            # generator losses
            contextual_loss_valid = tf.reduce_sum(tf.reduce_mean(
                tf.keras.losses.mean_absolute_error(y_true=real_images * batch_mask, y_pred=gen_output * batch_mask),
                axis=0))
            contextual_loss_hole = tf.reduce_sum(tf.reduce_mean(
                tf.keras.losses.mean_absolute_error(y_true=real_images * np.abs(1 - batch_mask),
                                                    y_pred=gen_output * np.abs(1 - batch_mask)), axis=0))
            contextual_loss = contextual_loss_valid + 6 * contextual_loss_hole
            perceptual_loss = tf.reduce_mean(
                tf.keras.losses.binary_crossentropy(y_true=tf.ones_like(dis_fake_output), y_pred=dis_fake_output))
            gen_loss = contextual_loss + 0.1 * perceptual_loss

            # discriminator losses
            dis_fake_loss = tf.reduce_mean(
                tf.keras.losses.binary_crossentropy(y_true=tf.zeros_like(dis_fake_output), y_pred=dis_fake_output))
            dis_real_loss = tf.reduce_mean(
                tf.keras.losses.binary_crossentropy(y_true=tf.ones_like(dis_real_output), y_pred=dis_real_output))
            dis_loss = dis_real_loss + dis_fake_loss

            # values for epoch summary
            dis_correct_real_preds_number = tf.count_nonzero(dis_real_output >= 0.5).numpy()
            dis_real_preds_len = len(np.array(dis_real_output))
            dis_real_accuracy = dis_correct_real_preds_number / dis_real_preds_len

            dis_correct_fake_preds_number = tf.count_nonzero(dis_fake_output < 0.5).numpy()
            dis_fake_preds_len = len(np.array(dis_fake_output))
            dis_fake_accuracy = dis_correct_fake_preds_number / dis_fake_preds_len

            gen_correct_preds_number = tf.count_nonzero(dis_fake_output >= 0.5).numpy()
            gen_preds_len = len(np.array(dis_fake_output))
            gen_accuracy = gen_correct_preds_number / gen_preds_len

        # also values for epoch summary
        gen_avg_loss += gen_loss.numpy()
        perceptual_accuracy += gen_accuracy
        dis_avg_real_accuracy += dis_real_accuracy
        dis_avg_fake_accuracy += dis_fake_accuracy

        gen_gradients = gen_tape.gradient(gen_loss, gen.trainable_variables)
        if episode < 5:
            dis_gradients = dis_tape.gradient(dis_loss, dis.trainable_variables)

        gen_optimizer.apply_gradients(zip(gen_gradients, gen.trainable_variables))
        if episode < 5:
            dis_optimizer.apply_gradients(zip(dis_gradients, dis.trainable_variables))

        if step % 10 == 9:
            print(".", end="")

    print("\nEpisode {}, dis acc: fake {} real {}, perceptual_acc {}, gen_loss: {}".
          format(episode, dis_avg_fake_accuracy / (step + 1), dis_avg_real_accuracy / (step + 1),
                 perceptual_accuracy / (step + 1), gen_avg_loss / (step + 1)))

# saving models weights
gen.save_weights(filepath="./generator.h5")
dis.save_weights(filepath="./discriminator.h5")

# see how it works on not seen images
(_, _), (test_x, _) = tf.keras.datasets.cifar10.load_data()
test_x = (test_x / 255.0).astype(dtype=np.float32)

test_x = tf.image.resize_images(test_x[:10], size=(64, 64))

masked_images = test_x[:6] * mask[:6] + (np.abs(mask[:6] - 1) / 2.0)

image = gen(masked_images, training=False)
for i, im in enumerate(image):
    plt.subplot(3, 6, i + 1)
    plt.imshow(im)

for i in range(6):
    plt.subplot(3, 6, i + 1 + 6)
    plt.imshow(masked_images[i])

for i in range(6):
    plt.subplot(3, 6, i + 1 + 12)
    plt.imshow(test_x[i])

plt.show()
